# How Attention affect VanillaCNN and ResNet10 


<!-- ABOUT THE PROJECT -->
## About The Project
For this assignment, in the first part, we're going to add the attention layer to the VanillaCNN model (Early, Late, Double). In the second part, we're going to add the attention layer to the Resnet10.

This project contains two parts.

* VanillaCNN with Attention
* ResNet10 with Attention





We train the network for 10 epochs and rank the accurancy for 
    
    "Vanilla CNN (No Attention)",
    "Vanilla CNN (Single Attention Block: Early attention)",
    "Vanilla CNN (Single Attention Block: Late attention)",
    "Vanilla CNN (Double Attention Blocks)",
    "ResNet10",
    "ResNet10 with Attention"


![accuracy rank](https://github.com/tonytonyhsiao/intro-to-visual-learning_hw4/blob/main/rank%20diagram.png)

