{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw6n8sqlFIjZ",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Assignment 4: Self-Attention for Vision\n",
    "\n",
    "For this assignment, we're going to implement self-attention blocks in a convolutional neural network for CIFAR-10 Classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CWmQwMDlFIjj"
   },
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LhcP-S1VFIjj",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134,
     "referenced_widgets": [
      "6646a364fe5b431eb578707bae8329b8",
      "92a914b698524f5ebd49603d108fbff9",
      "149c512369044de1951159ab8d9ec5ed",
      "15e4679b552e4d9c85a490b59222d1cf",
      "e7cb787fa0d743c08b5d4c395a7431f7",
      "e3796ddf7334425dbc86e68ff37d8a18",
      "d080590971d14b158aea92032dfebecc",
      "4dfce8d8475644e69a52e20410e4fd1b"
     ]
    },
    "id": "ZJJBVo0YFIjk",
    "outputId": "0ad37cdb-9bab-4f74-bdc5-0463ab7cbbad",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./data/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tony8\\\\Desktop\\\\ucsd 2024 spring\\\\ece285\\\\assignment4\\\\intro-to-visual-learning_hw4\\\\assignment4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "An3cfS0tFIjm",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOu2LLoEFIjm",
    "outputId": "683549ab-f649-4709-fd83-d44ef527e4df",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d0Cp2ir0FIjn",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Flatten Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4QkOBJdFIjo",
    "outputId": "286fe612-3115-42ad-a8aa-352adb9d74e2",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening:  tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XGHNkBu-FIjr"
   },
   "source": [
    "### Check Accuracy Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zeM908EmFIjv"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # useful stateless functions\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return 100 * acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ma0ntxxFIjv"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0kP6SeaiFIjv"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    acc_max = 0\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                acc = check_accuracy(loader_val, model)\n",
    "                if acc >= acc_max:\n",
    "                    acc_max = acc\n",
    "                print()\n",
    "    print(\"Maximum accuracy attained: \", acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla CNN; No Attention\n",
    "We implement the vanilla architecture for you here. Do not modify the architecture. You will use the same architecture in the following parts. Do not modify the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3075\n",
      "Checking accuracy on validation set\n",
      "Got 119 / 1000 correct (11.90)\n",
      "\n",
      "Epoch 0, Iteration 100, loss = 1.7947\n",
      "Checking accuracy on validation set\n",
      "Got 329 / 1000 correct (32.90)\n",
      "\n",
      "Epoch 0, Iteration 200, loss = 1.6213\n",
      "Checking accuracy on validation set\n",
      "Got 421 / 1000 correct (42.10)\n",
      "\n",
      "Epoch 0, Iteration 300, loss = 1.3957\n",
      "Checking accuracy on validation set\n",
      "Got 435 / 1000 correct (43.50)\n",
      "\n",
      "Epoch 0, Iteration 400, loss = 1.6808\n",
      "Checking accuracy on validation set\n",
      "Got 473 / 1000 correct (47.30)\n",
      "\n",
      "Epoch 0, Iteration 500, loss = 1.1844\n",
      "Checking accuracy on validation set\n",
      "Got 495 / 1000 correct (49.50)\n",
      "\n",
      "Epoch 0, Iteration 600, loss = 1.5089\n",
      "Checking accuracy on validation set\n",
      "Got 492 / 1000 correct (49.20)\n",
      "\n",
      "Epoch 0, Iteration 700, loss = 1.5990\n",
      "Checking accuracy on validation set\n",
      "Got 496 / 1000 correct (49.60)\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 1.2993\n",
      "Checking accuracy on validation set\n",
      "Got 484 / 1000 correct (48.40)\n",
      "\n",
      "Epoch 1, Iteration 100, loss = 1.4215\n",
      "Checking accuracy on validation set\n",
      "Got 509 / 1000 correct (50.90)\n",
      "\n",
      "Epoch 1, Iteration 200, loss = 1.3876\n",
      "Checking accuracy on validation set\n",
      "Got 509 / 1000 correct (50.90)\n",
      "\n",
      "Epoch 1, Iteration 300, loss = 1.2939\n",
      "Checking accuracy on validation set\n",
      "Got 501 / 1000 correct (50.10)\n",
      "\n",
      "Epoch 1, Iteration 400, loss = 1.2927\n",
      "Checking accuracy on validation set\n",
      "Got 502 / 1000 correct (50.20)\n",
      "\n",
      "Epoch 1, Iteration 500, loss = 1.2360\n",
      "Checking accuracy on validation set\n",
      "Got 541 / 1000 correct (54.10)\n",
      "\n",
      "Epoch 1, Iteration 600, loss = 1.2606\n",
      "Checking accuracy on validation set\n",
      "Got 545 / 1000 correct (54.50)\n",
      "\n",
      "Epoch 1, Iteration 700, loss = 1.3474\n",
      "Checking accuracy on validation set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 1.1012\n",
      "Checking accuracy on validation set\n",
      "Got 565 / 1000 correct (56.50)\n",
      "\n",
      "Epoch 2, Iteration 100, loss = 1.0421\n",
      "Checking accuracy on validation set\n",
      "Got 559 / 1000 correct (55.90)\n",
      "\n",
      "Epoch 2, Iteration 200, loss = 1.2294\n",
      "Checking accuracy on validation set\n",
      "Got 548 / 1000 correct (54.80)\n",
      "\n",
      "Epoch 2, Iteration 300, loss = 1.4021\n",
      "Checking accuracy on validation set\n",
      "Got 553 / 1000 correct (55.30)\n",
      "\n",
      "Epoch 2, Iteration 400, loss = 1.0690\n",
      "Checking accuracy on validation set\n",
      "Got 550 / 1000 correct (55.00)\n",
      "\n",
      "Epoch 2, Iteration 500, loss = 1.3158\n",
      "Checking accuracy on validation set\n",
      "Got 567 / 1000 correct (56.70)\n",
      "\n",
      "Epoch 2, Iteration 600, loss = 1.1403\n",
      "Checking accuracy on validation set\n",
      "Got 535 / 1000 correct (53.50)\n",
      "\n",
      "Epoch 2, Iteration 700, loss = 1.4237\n",
      "Checking accuracy on validation set\n",
      "Got 552 / 1000 correct (55.20)\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 1.1836\n",
      "Checking accuracy on validation set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "\n",
      "Epoch 3, Iteration 100, loss = 1.2548\n",
      "Checking accuracy on validation set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "\n",
      "Epoch 3, Iteration 200, loss = 0.9285\n",
      "Checking accuracy on validation set\n",
      "Got 543 / 1000 correct (54.30)\n",
      "\n",
      "Epoch 3, Iteration 300, loss = 1.0155\n",
      "Checking accuracy on validation set\n",
      "Got 550 / 1000 correct (55.00)\n",
      "\n",
      "Epoch 3, Iteration 400, loss = 1.2717\n",
      "Checking accuracy on validation set\n",
      "Got 547 / 1000 correct (54.70)\n",
      "\n",
      "Epoch 3, Iteration 500, loss = 1.1906\n",
      "Checking accuracy on validation set\n",
      "Got 559 / 1000 correct (55.90)\n",
      "\n",
      "Epoch 3, Iteration 600, loss = 1.0717\n",
      "Checking accuracy on validation set\n",
      "Got 560 / 1000 correct (56.00)\n",
      "\n",
      "Epoch 3, Iteration 700, loss = 1.1196\n",
      "Checking accuracy on validation set\n",
      "Got 558 / 1000 correct (55.80)\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 0.9992\n",
      "Checking accuracy on validation set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "\n",
      "Epoch 4, Iteration 100, loss = 1.0290\n",
      "Checking accuracy on validation set\n",
      "Got 555 / 1000 correct (55.50)\n",
      "\n",
      "Epoch 4, Iteration 200, loss = 1.3032\n",
      "Checking accuracy on validation set\n",
      "Got 543 / 1000 correct (54.30)\n",
      "\n",
      "Epoch 4, Iteration 300, loss = 0.9391\n",
      "Checking accuracy on validation set\n",
      "Got 558 / 1000 correct (55.80)\n",
      "\n",
      "Epoch 4, Iteration 400, loss = 1.2673\n",
      "Checking accuracy on validation set\n",
      "Got 568 / 1000 correct (56.80)\n",
      "\n",
      "Epoch 4, Iteration 500, loss = 1.1165\n",
      "Checking accuracy on validation set\n",
      "Got 571 / 1000 correct (57.10)\n",
      "\n",
      "Epoch 4, Iteration 600, loss = 1.3937\n",
      "Checking accuracy on validation set\n",
      "Got 563 / 1000 correct (56.30)\n",
      "\n",
      "Epoch 4, Iteration 700, loss = 1.0703\n",
      "Checking accuracy on validation set\n",
      "Got 577 / 1000 correct (57.70)\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 0.9521\n",
      "Checking accuracy on validation set\n",
      "Got 560 / 1000 correct (56.00)\n",
      "\n",
      "Epoch 5, Iteration 100, loss = 1.0201\n",
      "Checking accuracy on validation set\n",
      "Got 565 / 1000 correct (56.50)\n",
      "\n",
      "Epoch 5, Iteration 200, loss = 1.0070\n",
      "Checking accuracy on validation set\n",
      "Got 575 / 1000 correct (57.50)\n",
      "\n",
      "Epoch 5, Iteration 300, loss = 0.8114\n",
      "Checking accuracy on validation set\n",
      "Got 565 / 1000 correct (56.50)\n",
      "\n",
      "Epoch 5, Iteration 400, loss = 1.0520\n",
      "Checking accuracy on validation set\n",
      "Got 560 / 1000 correct (56.00)\n",
      "\n",
      "Epoch 5, Iteration 500, loss = 0.9986\n",
      "Checking accuracy on validation set\n",
      "Got 570 / 1000 correct (57.00)\n",
      "\n",
      "Epoch 5, Iteration 600, loss = 1.0014\n",
      "Checking accuracy on validation set\n",
      "Got 577 / 1000 correct (57.70)\n",
      "\n",
      "Epoch 5, Iteration 700, loss = 0.9659\n",
      "Checking accuracy on validation set\n",
      "Got 568 / 1000 correct (56.80)\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 0.8675\n",
      "Checking accuracy on validation set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "\n",
      "Epoch 6, Iteration 100, loss = 1.1018\n",
      "Checking accuracy on validation set\n",
      "Got 543 / 1000 correct (54.30)\n",
      "\n",
      "Epoch 6, Iteration 200, loss = 0.8751\n",
      "Checking accuracy on validation set\n",
      "Got 565 / 1000 correct (56.50)\n",
      "\n",
      "Epoch 6, Iteration 300, loss = 1.1777\n",
      "Checking accuracy on validation set\n",
      "Got 574 / 1000 correct (57.40)\n",
      "\n",
      "Epoch 6, Iteration 400, loss = 0.8831\n",
      "Checking accuracy on validation set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "\n",
      "Epoch 6, Iteration 500, loss = 1.0549\n",
      "Checking accuracy on validation set\n",
      "Got 563 / 1000 correct (56.30)\n",
      "\n",
      "Epoch 6, Iteration 600, loss = 0.9542\n",
      "Checking accuracy on validation set\n",
      "Got 577 / 1000 correct (57.70)\n",
      "\n",
      "Epoch 6, Iteration 700, loss = 0.6316\n",
      "Checking accuracy on validation set\n",
      "Got 567 / 1000 correct (56.70)\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 1.0298\n",
      "Checking accuracy on validation set\n",
      "Got 556 / 1000 correct (55.60)\n",
      "\n",
      "Epoch 7, Iteration 100, loss = 0.6371\n",
      "Checking accuracy on validation set\n",
      "Got 580 / 1000 correct (58.00)\n",
      "\n",
      "Epoch 7, Iteration 200, loss = 0.8155\n",
      "Checking accuracy on validation set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "\n",
      "Epoch 7, Iteration 300, loss = 0.9577\n",
      "Checking accuracy on validation set\n",
      "Got 577 / 1000 correct (57.70)\n",
      "\n",
      "Epoch 7, Iteration 400, loss = 0.6710\n",
      "Checking accuracy on validation set\n",
      "Got 568 / 1000 correct (56.80)\n",
      "\n",
      "Epoch 7, Iteration 500, loss = 0.9266\n",
      "Checking accuracy on validation set\n",
      "Got 570 / 1000 correct (57.00)\n",
      "\n",
      "Epoch 7, Iteration 600, loss = 0.5600\n",
      "Checking accuracy on validation set\n",
      "Got 543 / 1000 correct (54.30)\n",
      "\n",
      "Epoch 7, Iteration 700, loss = 1.0255\n",
      "Checking accuracy on validation set\n",
      "Got 581 / 1000 correct (58.10)\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 0.7263\n",
      "Checking accuracy on validation set\n",
      "Got 582 / 1000 correct (58.20)\n",
      "\n",
      "Epoch 8, Iteration 100, loss = 0.8273\n",
      "Checking accuracy on validation set\n",
      "Got 563 / 1000 correct (56.30)\n",
      "\n",
      "Epoch 8, Iteration 200, loss = 0.8593\n",
      "Checking accuracy on validation set\n",
      "Got 577 / 1000 correct (57.70)\n",
      "\n",
      "Epoch 8, Iteration 300, loss = 0.8411\n",
      "Checking accuracy on validation set\n",
      "Got 574 / 1000 correct (57.40)\n",
      "\n",
      "Epoch 8, Iteration 400, loss = 0.7489\n",
      "Checking accuracy on validation set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "\n",
      "Epoch 8, Iteration 500, loss = 0.6574\n",
      "Checking accuracy on validation set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "\n",
      "Epoch 8, Iteration 600, loss = 0.7573\n",
      "Checking accuracy on validation set\n",
      "Got 571 / 1000 correct (57.10)\n",
      "\n",
      "Epoch 8, Iteration 700, loss = 1.0959\n",
      "Checking accuracy on validation set\n",
      "Got 568 / 1000 correct (56.80)\n",
      "\n",
      "Epoch 9, Iteration 0, loss = 0.6732\n",
      "Checking accuracy on validation set\n",
      "Got 572 / 1000 correct (57.20)\n",
      "\n",
      "Epoch 9, Iteration 100, loss = 0.8854\n",
      "Checking accuracy on validation set\n",
      "Got 578 / 1000 correct (57.80)\n",
      "\n",
      "Epoch 9, Iteration 200, loss = 0.7693\n",
      "Checking accuracy on validation set\n",
      "Got 575 / 1000 correct (57.50)\n",
      "\n",
      "Epoch 9, Iteration 300, loss = 0.6084\n",
      "Checking accuracy on validation set\n",
      "Got 582 / 1000 correct (58.20)\n",
      "\n",
      "Epoch 9, Iteration 400, loss = 0.5895\n",
      "Checking accuracy on validation set\n",
      "Got 581 / 1000 correct (58.10)\n",
      "\n",
      "Epoch 9, Iteration 500, loss = 0.8550\n",
      "Checking accuracy on validation set\n",
      "Got 562 / 1000 correct (56.20)\n",
      "\n",
      "Epoch 9, Iteration 600, loss = 0.9844\n",
      "Checking accuracy on validation set\n",
      "Got 585 / 1000 correct (58.50)\n",
      "\n",
      "Epoch 9, Iteration 700, loss = 0.6877\n",
      "Checking accuracy on validation set\n",
      "Got 574 / 1000 correct (57.40)\n",
      "\n",
      "Maximum accuracy attained:  58.5\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "num_classes = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, 3, padding=1, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32, num_classes),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
    "You should be able to see atleast 55% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 5787 / 10000 correct (57.87)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanillaModel = model\n",
    "check_accuracy(loader_test, vanillaModel)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-gH8-iqGUOO1"
   },
   "source": [
    "## Part II Self-Attention\n",
    "\n",
    "In the next section, you will implement an Attention layer which you will then use within a convnet architecture defined above for cifar 10 classification task.\n",
    "\n",
    "A self-attention layer is formulated as following:\n",
    "\n",
    "Input: $X$ of shape $(H\\times W, C)$\n",
    "\n",
    "Query, key, value linear transforms are $W_Q$, $W_K$, $W_V$, of shape $(C, C)$. We implement these linear transforms as 1x1 convolutional layers of the same dimensions.\n",
    "\n",
    "$XW_Q$, $XW_K$, $XW_V$, represent the output volumes when input X is passed through the transforms.\n",
    "\n",
    "\n",
    "Self-Attention is given by the formula: $Attention(X) = X + Softmax(\\frac{XW_Q(XW_K)^\\top}{\\sqrt{C}})XW_V$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 1: Self-Attention is equivalent to which of the following: (5 points)\n",
    "1. K-means clustering <br />\n",
    "2. Non-local means <br />\n",
    "3. Residual Block <br />\n",
    "4. Gaussian Blurring <br />\n",
    "\n",
    "Your Answer: Non-local means. Because self-attention is a mechanism where each element of a sequence attends to all other elements, computing a weighted sum based on a similarity measure (such as dot product). This is similar to the Non-local means algorithm, which enhances each pixel based on a weighted average of all other pixels, with weights determined by the similarity between local patches. Hence, Self-Attention is conceptually equivalent to Non-local means."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you implement the Attention module, and run it in the next section (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uvcTf4eUY-FX"
   },
   "outputs": [],
   "source": [
    "# Initialize the attention module as a nn.Module subclass\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Implement the Key, Query and Value linear transforms as 1x1 convolutional layers\n",
    "        # Hint: channel size remains constant throughout\n",
    "        self.conv_query = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv_key = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv_value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        # TODO: Pass the input through conv_query, reshape the output volume to (N, C, H*W)\n",
    "        q = self.conv_query(x)\n",
    "        q = q.reshape(N, C, H*W)\n",
    "        # TODO: Pass the input through conv_key, reshape the output volume to (N, C, H*W)\n",
    "        k = self.conv_key(x)\n",
    "        k = k.reshape(N, C, H*W)\n",
    "        \n",
    "        # TODO: Pass the input through conv_value, reshape the output volume to (N, C, H*W)\n",
    "        v = self.conv_value(x)\n",
    "        v = v.reshape(N, C, H*W)\n",
    "        # TODO: Implement the above formula for attention using q, k, v, C\n",
    "        # NOTE: The X in the formula is already added for you in the return line\n",
    "        temp=torch.sqrt(torch.Tensor([C]))\n",
    "        attention =F.softmax(q @ torch.transpose(k,1,2)/temp, dim=-1)\n",
    "        attention=attention@v\n",
    "        # Reshape the output to (N, C, H, W) before adding to the input volume\n",
    "        attention = attention.reshape(N, C, H, W)\n",
    "        return x + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Attention Block: Early attention; After the first conv layer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6A3xh7aVFWzM",
    "outputId": "f45f9049-70ce-414f-ab1d-446f619ad367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 2.3151\n",
      "Checking accuracy on validation set\n",
      "Got 118 / 1000 correct (11.80)\n",
      "\n",
      "Epoch 0, Iteration 100, loss = 1.7077\n",
      "Checking accuracy on validation set\n",
      "Got 431 / 1000 correct (43.10)\n",
      "\n",
      "Epoch 0, Iteration 200, loss = 1.2479\n",
      "Checking accuracy on validation set\n",
      "Got 512 / 1000 correct (51.20)\n",
      "\n",
      "Epoch 0, Iteration 300, loss = 1.3700\n",
      "Checking accuracy on validation set\n",
      "Got 526 / 1000 correct (52.60)\n",
      "\n",
      "Epoch 0, Iteration 400, loss = 1.1930\n",
      "Checking accuracy on validation set\n",
      "Got 557 / 1000 correct (55.70)\n",
      "\n",
      "Epoch 0, Iteration 500, loss = 1.1429\n",
      "Checking accuracy on validation set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "\n",
      "Epoch 0, Iteration 600, loss = 1.4600\n",
      "Checking accuracy on validation set\n",
      "Got 572 / 1000 correct (57.20)\n",
      "\n",
      "Epoch 0, Iteration 700, loss = 1.0842\n",
      "Checking accuracy on validation set\n",
      "Got 605 / 1000 correct (60.50)\n",
      "\n",
      "Epoch 1, Iteration 0, loss = 0.9772\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n",
      "Epoch 1, Iteration 100, loss = 1.1238\n",
      "Checking accuracy on validation set\n",
      "Got 609 / 1000 correct (60.90)\n",
      "\n",
      "Epoch 1, Iteration 200, loss = 0.8994\n",
      "Checking accuracy on validation set\n",
      "Got 619 / 1000 correct (61.90)\n",
      "\n",
      "Epoch 1, Iteration 300, loss = 0.7786\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Epoch 1, Iteration 400, loss = 1.0248\n",
      "Checking accuracy on validation set\n",
      "Got 623 / 1000 correct (62.30)\n",
      "\n",
      "Epoch 1, Iteration 500, loss = 0.9956\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Epoch 1, Iteration 600, loss = 0.9010\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Epoch 1, Iteration 700, loss = 0.9912\n",
      "Checking accuracy on validation set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "\n",
      "Epoch 2, Iteration 0, loss = 0.6693\n",
      "Checking accuracy on validation set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "\n",
      "Epoch 2, Iteration 100, loss = 0.7841\n",
      "Checking accuracy on validation set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "\n",
      "Epoch 2, Iteration 200, loss = 0.6054\n",
      "Checking accuracy on validation set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "\n",
      "Epoch 2, Iteration 300, loss = 0.7187\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Epoch 2, Iteration 400, loss = 0.6168\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Epoch 2, Iteration 500, loss = 0.8308\n",
      "Checking accuracy on validation set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "\n",
      "Epoch 2, Iteration 600, loss = 0.8789\n",
      "Checking accuracy on validation set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "\n",
      "Epoch 2, Iteration 700, loss = 0.7581\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Epoch 3, Iteration 0, loss = 0.5594\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Epoch 3, Iteration 100, loss = 0.6447\n",
      "Checking accuracy on validation set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "\n",
      "Epoch 3, Iteration 200, loss = 0.5275\n",
      "Checking accuracy on validation set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "\n",
      "Epoch 3, Iteration 300, loss = 0.6892\n",
      "Checking accuracy on validation set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "\n",
      "Epoch 3, Iteration 400, loss = 0.3884\n",
      "Checking accuracy on validation set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "\n",
      "Epoch 3, Iteration 500, loss = 0.8189\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Epoch 3, Iteration 600, loss = 0.5419\n",
      "Checking accuracy on validation set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "\n",
      "Epoch 3, Iteration 700, loss = 0.7522\n",
      "Checking accuracy on validation set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "\n",
      "Epoch 4, Iteration 0, loss = 0.5086\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Epoch 4, Iteration 100, loss = 0.4279\n",
      "Checking accuracy on validation set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "\n",
      "Epoch 4, Iteration 200, loss = 0.3348\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Epoch 4, Iteration 300, loss = 0.4936\n",
      "Checking accuracy on validation set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "\n",
      "Epoch 4, Iteration 400, loss = 0.3003\n",
      "Checking accuracy on validation set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "\n",
      "Epoch 4, Iteration 500, loss = 0.4002\n",
      "Checking accuracy on validation set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "\n",
      "Epoch 4, Iteration 600, loss = 0.5044\n",
      "Checking accuracy on validation set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "\n",
      "Epoch 4, Iteration 700, loss = 0.4589\n",
      "Checking accuracy on validation set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "\n",
      "Epoch 5, Iteration 0, loss = 0.2427\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Epoch 5, Iteration 100, loss = 0.3316\n",
      "Checking accuracy on validation set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "\n",
      "Epoch 5, Iteration 200, loss = 0.3463\n",
      "Checking accuracy on validation set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "\n",
      "Epoch 5, Iteration 300, loss = 0.4245\n",
      "Checking accuracy on validation set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "\n",
      "Epoch 5, Iteration 400, loss = 0.2485\n",
      "Checking accuracy on validation set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "\n",
      "Epoch 5, Iteration 500, loss = 0.2442\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Epoch 5, Iteration 600, loss = 0.4370\n",
      "Checking accuracy on validation set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "\n",
      "Epoch 5, Iteration 700, loss = 0.3923\n",
      "Checking accuracy on validation set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "\n",
      "Epoch 6, Iteration 0, loss = 0.1861\n",
      "Checking accuracy on validation set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "\n",
      "Epoch 6, Iteration 100, loss = 0.1651\n",
      "Checking accuracy on validation set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "\n",
      "Epoch 6, Iteration 200, loss = 0.1347\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Epoch 6, Iteration 300, loss = 0.1990\n",
      "Checking accuracy on validation set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Epoch 6, Iteration 400, loss = 0.2374\n",
      "Checking accuracy on validation set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "\n",
      "Epoch 6, Iteration 500, loss = 0.2862\n",
      "Checking accuracy on validation set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "\n",
      "Epoch 6, Iteration 600, loss = 0.1907\n",
      "Checking accuracy on validation set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "\n",
      "Epoch 6, Iteration 700, loss = 0.1698\n",
      "Checking accuracy on validation set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "\n",
      "Epoch 7, Iteration 0, loss = 0.2119\n",
      "Checking accuracy on validation set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "\n",
      "Epoch 7, Iteration 100, loss = 0.1283\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Epoch 7, Iteration 200, loss = 0.1874\n",
      "Checking accuracy on validation set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "\n",
      "Epoch 7, Iteration 300, loss = 0.1202\n",
      "Checking accuracy on validation set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "\n",
      "Epoch 7, Iteration 400, loss = 0.0812\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Epoch 7, Iteration 500, loss = 0.2464\n",
      "Checking accuracy on validation set\n",
      "Got 621 / 1000 correct (62.10)\n",
      "\n",
      "Epoch 7, Iteration 600, loss = 0.1045\n",
      "Checking accuracy on validation set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "\n",
      "Epoch 7, Iteration 700, loss = 0.2310\n",
      "Checking accuracy on validation set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "\n",
      "Epoch 8, Iteration 0, loss = 0.0817\n",
      "Checking accuracy on validation set\n",
      "Got 625 / 1000 correct (62.50)\n",
      "\n",
      "Epoch 8, Iteration 100, loss = 0.1416\n",
      "Checking accuracy on validation set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "\n",
      "Epoch 8, Iteration 200, loss = 0.1186\n",
      "Checking accuracy on validation set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "\n",
      "Epoch 8, Iteration 300, loss = 0.1656\n",
      "Checking accuracy on validation set\n",
      "Got 621 / 1000 correct (62.10)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m3\u001b[39m,channel_1,\u001b[38;5;241m3\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(channel_2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, epochs)\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# This is the backwards pass: compute the gradient of the loss with\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# respect to each  parameter of the model.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Actually update the parameters of the model using the gradients\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# computed by the backwards pass.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ece285hw\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ece285hw\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the first Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Linear]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,channel_1,3,padding=1,stride=1),\n",
    "    nn.ReLU(),\n",
    "    Attention(channel_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1,channel_2,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32,10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
    "You should see improvement of about 2-3% over the vanilla convnet model. * Use this part to tune your Attention module and then move on to the next parts. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "earlyAttention = model\n",
    "check_accuracy(loader_test, earlyAttention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Attention Block: Late attention; After the second conv layer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Conv->Relu->Attention->Relu->Linear]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,channel_1,3,padding=1,stride=1),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    \n",
    "    nn.Conv2d(channel_1,channel_2,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    Attention(channel_2),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32,10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "lateAttention = model\n",
    "check_accuracy(loader_test, lateAttention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 2: Provide one example each of usage of self-attention and attention in computer vision. Explain the difference between the two. (5 points)\n",
    "\n",
    "\n",
    "Your Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Attention Blocks: After conv layers 1 and 2 (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 64\n",
    "channel_2 = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
    "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Attention->Relu->Linear]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,channel_1,3,padding=1,stride=1),\n",
    "    nn.ReLU(),\n",
    "    Attention(channel_1),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(channel_1,channel_2,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    Attention(channel_2),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32,10),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cTw89bPoFIjy"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpj3j6aEFIjy",
    "outputId": "2220f2f2-b24d-4c1d-9d9a-ed212567fce9"
   },
   "outputs": [],
   "source": [
    "vanillaModel = model\n",
    "check_accuracy(loader_test, vanillaModel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet with Attention \n",
    "\n",
    "Now we will experiment with applying attention within the Resnet10 architecture that we implemented in Homework 2. Please note that for a deeper model such as Resnet we do not expect significant improvements in performance with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Resnet, No Attention\n",
    "\n",
    "The architecture for Resnet is given below, please train it and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, img_channels=3, num_classes=100, batchnorm=False):\n",
    "        super(ResNet, self).__init__() #layers = [1, 1, 1, 1] \n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.batchnorm = batchnorm\n",
    "        self.layer1 = self.make_layer(block, layers[0], out_channels=64, stride=1, batchnorm=batchnorm)\n",
    "        self.layer2 = self.make_layer(block, layers[1], out_channels=128, stride=1, batchnorm=batchnorm)\n",
    "        self.layer3 = self.make_layer(block, layers[2], out_channels=256, stride=1, batchnorm=batchnorm)\n",
    "        self.layer4 = self.make_layer(block, layers[3], out_channels=512, stride=2, batchnorm=batchnorm)\n",
    "\n",
    "        self.averagepool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x) \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.averagepool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def make_layer(self, block, num_blocks, out_channels, stride, batchnorm=False):\n",
    "        downsampler = None\n",
    "        layers = []\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsampler = nn.Sequential(nn.Conv2d(self.in_channels, out_channels, kernel_size = 1, stride = stride), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.append(block(self.in_channels, out_channels, downsampler, stride, batchnorm=batchnorm))\n",
    "\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        for i in range(num_blocks - 1):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "class block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, downsampler = None, stride = 1, batchnorm=False):\n",
    "        \n",
    "        super(block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsampler = downsampler\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        if self.downsampler:\n",
    "            residual = self.downsampler(residual)\n",
    "\n",
    "        return self.relu(residual + x)\n",
    "    \n",
    "\n",
    "\n",
    "def ResNet10(num_classes = 100, batchnorm= False):\n",
    "\n",
    "    return ResNet(block, [1, 1, 1, 1], num_classes=num_classes, batchnorm=batchnorm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "model = ResNet10()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)\n",
    "\n",
    "vanillaResnet = model\n",
    "check_accuracy(loader_test, vanillaResnet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet with Attention (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resnet with Attention\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# TODO: Use the above Attention module after the 2nd resnet block i.e. after self.layer2.\n",
    "\n",
    "model = None\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionResnet = model\n",
    "check_accuracy(loader_test, AttentionResnet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 3: Rank the above models based on their performance on test dataset (15 points)\n",
    "( You are encouraged to run each of the experiments (training) at\n",
    "least 3 times to get an average estimate )\n",
    "\n",
    "Report the test accuracies alongside the model names. For example, 1. Vanilla CNN (57.45%, 57.99%).. etc\n",
    "\n",
    "1. <br />\n",
    "2. <br />\n",
    "3. <br />\n",
    "4. <br />\n",
    "5. <br />\n",
    "6. <br />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question (Ungraded): Can you give a possible explanation that supports the rankings?\n",
    "Your Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "assignment3_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "149c512369044de1951159ab8d9ec5ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3796ddf7334425dbc86e68ff37d8a18",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7cb787fa0d743c08b5d4c395a7431f7",
      "value": 170498071
     }
    },
    "15e4679b552e4d9c85a490b59222d1cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dfce8d8475644e69a52e20410e4fd1b",
      "placeholder": "​",
      "style": "IPY_MODEL_d080590971d14b158aea92032dfebecc",
      "value": " 170499072/? [00:18&lt;00:00, 9456903.71it/s]"
     }
    },
    "4dfce8d8475644e69a52e20410e4fd1b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6646a364fe5b431eb578707bae8329b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_149c512369044de1951159ab8d9ec5ed",
       "IPY_MODEL_15e4679b552e4d9c85a490b59222d1cf"
      ],
      "layout": "IPY_MODEL_92a914b698524f5ebd49603d108fbff9"
     }
    },
    "92a914b698524f5ebd49603d108fbff9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d080590971d14b158aea92032dfebecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3796ddf7334425dbc86e68ff37d8a18": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7cb787fa0d743c08b5d4c395a7431f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
